{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNIwe5N7s0e_"
   },
   "source": [
    "# DATA WRANGLING AND COMBINING TWO DATASETS IN PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BG63Tpg8ep_"
   },
   "source": [
    "In this project, I will apply the skills  to gather and wrangle real-world data with two datasets of my choice.\n",
    "\n",
    "You will retrieve and extract the data, assess the data programmatically and visually, accross elements of data quality and structure, and implement a cleaning strategy for the data. You will then store the updated data into your selected database/data store, combine the data, and answer a research question with the datasets.\n",
    "\n",
    "Throughout the process, I will:\n",
    "\n",
    "1. Explain your decisions towards methods used for gathering, assessing, cleaning, storing, and answering the research question\n",
    "2. Write code comments so code is more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDYDkH-Zs7Nn"
   },
   "source": [
    "## 1. Gather data\n",
    "\n",
    "In this section, you will extract data using two different data gathering methods and combine the data. Use at least two different types of data-gathering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbN7z7rcuqpO"
   },
   "source": [
    "### **1.1.** Problem Statement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gi6swhjSYqu2"
   },
   "source": [
    "In this Project the objective is to observe crash and collision data of two big cities in the United States. Chicago and New York. The objective is to determine: \n",
    "- Which city has a higher kill count when crashes occur, Chicago or New york city?\n",
    "\n",
    "- Between both Chicago and NewYork. Which date or time of the year has recorded the highest fatalaties in general between both cities?\n",
    "\n",
    "###### The data that will be used and the source\n",
    "The two datasets that will be used for this project can be found on the [Catalog-data-.gov](https://catalog.data.gov/dataset/motor-vehicle-collisions-crashes) website. \n",
    "\n",
    "1. The Chicago dateset which can be found here: [Chicago-motor-vehicle-crashes](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95)\n",
    "\n",
    "2. The NewYork dataset which can be found here: [NewYork-motor-vehicle-collisions](https://catalog.data.gov/dataset/motor-vehicle-collisions-crashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AQfBAdUypMm"
   },
   "source": [
    "### **1.2.** Different data gathering methods\n",
    "\n",
    "List of data gathering methods:\n",
    "\n",
    "- Download data manually\n",
    "- Programmatically downloading files\n",
    "- Gather data by accessing APIs\n",
    "- Gather and extract data from HTML files using BeautifulSoup\n",
    "- Extract data from a SQL database\n",
    "\n",
    "\n",
    "#### Data Gathering Methods used for this Project. \n",
    "\n",
    "For this project the **TWO** Data Gathering methods that will be chosen from the list above for this project are:\n",
    "\n",
    "\n",
    "1. Download data manually\n",
    "\n",
    "2. Gather data by accessing APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEtting Things up\n",
    "\n",
    "  Here, the Libraries will be imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "import sklearn\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import plotly.express as px\n",
    "import missingno as msn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e6gS0wL1KTu"
   },
   "source": [
    "## **Dataset 1**\n",
    "\n",
    "The first dataset that will be used is the **Traffic Crahses in Chicago**. \n",
    "\n",
    "The data type file is a **CSV** file. The file name is `traffic-crashes-chicago.csv`\n",
    "\n",
    "#### Method of loading the data\n",
    "The method used here is to open the **CSV** file manually. The file has already been downloaded locally in the `downloads` folder. It it will be opened using Pandas load to CSV method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Su8E0uLuYkHU",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Using Pandas \n",
    "sub = os.path.join('datasets', 'traffic-crashes-chicago.csv')\n",
    "chicago_rawdata = pd.read_csv(sub)\n",
    "\n",
    "# Load the dataset\n",
    "chicago_rawdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above it can be seen that the `traffic-crashes-chicago.csv` dataset has been successfully loaded using Pandas. This is assigned to the variable `dataset_one`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Small Summary about Dataset 1\n",
    "\n",
    "- From above we picked this CHicago dataset because it is vast and the data seems more kept to date. May be being as Chicago is one of the biggest cities in the world its accident count must be pretty high so accident data must be vital. \n",
    "\n",
    "- The gathering method used from above was to download the dataset and load the `CSV` file locally from the PC. \n",
    "\n",
    "- SOme interesting variables picked up at a glance were `CRASH_DATE`, `LOCATION` and `CRASH_MONTH`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoUjq1tPzz7P"
   },
   "source": [
    "### Dataset 2\n",
    "\n",
    "The next dataset used will be **Motor Vehicle Collisions-Crashes in New York**. \n",
    "\n",
    "The datatype is a **JSON** file on a webpage. \n",
    "\n",
    "\n",
    "#### Method of loading the data\n",
    "Loading the data here will be slightly different from the previous method in Dataset 1. Here we will scrape the data from the API found on the webpage here [Motor-Vehicle-Collisions-NYC](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95)  which provides the data in **JSON** format. Then, the data will be converted to a Pandas Dataframe. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zT0QxRyYmm7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# THE URL Link where the data is located\n",
    "\n",
    "URL_link = 'https://data.cityofnewyork.us/resource/h9gi-nx95.json'\n",
    "\n",
    "# using Pandas to read into a JSON file\n",
    "\n",
    "newyork_rawdata_json = pd.read_json(URL_link)\n",
    "\n",
    "# Load the dataset\n",
    "newyork_rawdata_json.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Small summary of Dataset 2\n",
    "\n",
    "- When doing a project about accidents the first city I automatically searched for was Newyork. New York is the most robust city with the most traffic and a high population and high movement. So I had to include New york in my findings\n",
    "\n",
    "- The gathering method here as mentioned was download the data which a online `JSON` file on the website using a API. The data successfully downloaded on the machine. (Below I will make a optional step to save this data as a `CSV` locally0\n",
    "\n",
    "- Some interesting variables noticed at first glance were `crash_date`, `crash_time` and `numbers_of_persons_injured`\n",
    "\n",
    "- FInally, the New york dataset was not as big as the Chicago but it was still very fruitful with data and very much kept up to date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional data storing step:** Since *DATASET 1* was loaded manually, the dataset is already saved locally. *DATASET 2* was loaded from the *URL*. Therefore we will save the raw datasetlocally now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Optional: store the dataset locally from the downloaded Json file online\n",
    "newyork_rawdata_json.to_csv('datasets\\dataset_two.csv', index=False)\n",
    "\n",
    "# load downloaded csv file of raw data \n",
    "# This is now the raw data file\n",
    "newyork_rawdata = pd.read_csv('datasets\\dataset_two.csv')\n",
    "\n",
    "print ('Successfully created Local backup for Newyork data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the local data folder we now have our two raw datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwSWIVmotLgV"
   },
   "source": [
    "## 2. Assess data\n",
    "\n",
    "Assess the data according to data quality and tidiness metrics using the report below.\n",
    "\n",
    "List **two** data quality issues and **two** tidiness issues. Assess each data issue visually **and** programmatically, then briefly describe the issue you find.  **Make sure you include justifications for the methods you use for the assessment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adaK2iPNzVu4"
   },
   "source": [
    "### Quality Issue 1: Checking for Missing Data and dropping Uncessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Issue one Visually\n",
    "We are going to first inspect the two dataset variables by plotting visualizations. Here we will dirst the **First quality issue which is seeing how much missing data is present in the two datasets respectively**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpW59kh-zl8d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe visually for chicago dataset\n",
    "msn.bar(chicago_rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "msn.bar(newyork_rawdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qfcocStzsKg"
   },
   "source": [
    "\n",
    "We can see the missing data from each of the datasets in two different ways. WE see the columns with missing rows of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUALITY ISSUE ONE PROGRAMATICALLY\n",
    "\n",
    "###### Here we are going to check two things:\n",
    "\n",
    "1. The total sum of null values \n",
    "2. The info of the datasets such as non-null values, data types length of columns. This helps to determine which columns to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using Isnull.sum() to see see the actually columns of missing data in each dataframe\n",
    "chicago_rawdata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chicago_rawdata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create the threshold. WE cannot depend on magic numbers or bias. So we use a proportionate amount of 25%. Below we will find what is 25% of the total row count of the chicago dataset. Whatever the output is our threshold. SO this means any column with `NaN` values exceeding that threshold will be dropped!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For chicago data threshold\n",
    "25/100 * 708368 \n",
    "\n",
    "print(' Any column yielding a greater or equal number of missing values than `177092` will be dropped from the Chicago dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for New york data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newyork_rawdata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newyork_rawdata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it it was done for the Chicago dataset above. This will be done here again with the New york data to find the 25% threshold to use to determine which columns with too many `NaN` values will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For new york\n",
    "\n",
    "25/100 * 1000\n",
    "print(' Any column yielding a greater number of missing values than `250` will be dropped from the New york dataset. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Issue and Justification for Quality Issue One\n",
    "\n",
    "- From the above we plotted visualizations for both datasets visualizating the missing data. Secondly, we programmtically analyzed the missing values of each column. \n",
    "\n",
    "- The biggest question here to avoid Bias is : \"How the data will be dropped and columns will be dropped? \" with the most logical way possible. \n",
    "\n",
    "- Not columns will be used because as seen some columns have a lot of missing data so we deem them to be \"unecessary\"\n",
    "\n",
    "- Using `info()` showed us the range index of the column which showed us the amount of rows in the column. So for example. For chicago data we saw the `figure` of `708268` that meant there are `708268` rows in each column. The same applies for New york there are `1000` rows of data in each column. \n",
    "\n",
    "- From knowing this information we will use a threshold. IF more than 25% of the data is missing or in NAN we drop the column completely. So whatever column yields that number greater it will be dropped. See code below for logic to negate magic numbers and form our threshold numbers to drop columns. \n",
    "- Finally, the columns that were not dropped will be considered useful and have their missing data dropped using `dropna()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Be77N4I1AmE"
   },
   "source": [
    "### Quality Issue 2 : CANNOT IDENTIFY THE DIFFERENCE BETWEEN THE TWO DATASETS BY CITY. There is no City Name within either of the datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us just think ahead for a quick moment. If we were to merge the two datasets at the moment. How would we be able to distinguish the data? For example how would we know which crash happened in Newyork while one happened in CHicago. Yes we have the longitude and lattitude but there is no indication helping to identify the city. If a reader was to read the dataset. THey would have no idea which city it is from. This is a big quality issue and needs to be attended too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnviRCUI-bb7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe visually via heatmaps \n",
    "\n",
    "# First chicago data\n",
    "msn.heatmap(chicago_rawdata, cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizting new york city columns via Heatmap\n",
    "msn.heatmap(newyork_rawdata, cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quality Issue 2 Programatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE have been looking at the two datasets for some time now to not notice that there is no any mention about a city name anywhere. Even from the visualizations above we do not seen any column pertaining to or mentioning anything to do with city name. So how can you distinguish the datasets by city. Here we will have create a column to help with this. \n",
    "\n",
    "**Below is a programmtical test example using a test dataframe of how the column will be created and will set the expectation on what to expect for the Chicago and New york datasets respectively**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating a test to programtically show \n",
    "\n",
    "test_data_1 = {'name': ['Willard Morris', 'Al Jennings', 'Omar Mullins', 'Spencer McDaniel'],\n",
    "'age': [20, 19, 22, 21],\n",
    "'favorite_color': ['blue', 'red', 'yellow', \"green\"],\n",
    "'grade': [88, 92, 95, 70]}\n",
    "test_df_1 = pd.DataFrame(test_data_1, index = ['Willard Morris', 'Al Jennings', 'Omar Mullins', 'Spencer McDaniel'])\n",
    "\n",
    "\n",
    "# Creating/Adding the new column as a test\n",
    "test_df_1['City Name'] = 'City Name Here'\n",
    "test_df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AS seen there is a constant value in the new test column city. It is required for the data to be constant for the analysis.\n",
    "\n",
    "Remember this is just an example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Issue and Justification of Quality Issue 2\n",
    "\n",
    "- From the heatmaps above we can see the column names clearly. If the data was going to be observed from any reader or third person they will not know this crash data was from New york city or Chicago city. \n",
    "\n",
    "- Taking it further checking the column list of names of the datasets. There is no indication stating the city name\n",
    "\n",
    "- The solution to this would be add a new column with a constant value the column name could be CityName and help distinguish and explicitly state the city name. So for example having a column ename **City** in each dataset and having the data be `NEwyork` and `Chicago` respectively. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXhGiYyiwwKN"
   },
   "source": [
    "### Tidiness Issue 1: The `CRASH_DATE` column in the Chicago dataset has both date and time in it. Secondly, the Newyork dataset has two date and time columns but one is unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The objective here is to keep the data separate from the time. It is confusing with both date and time in the same column. \n",
    "\n",
    "We will first visually display the `CRASH_DATE` column in a visualization (boxplot) to actually get a visual understanding of how the data actually looks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# FILL IN - Inspecting the data visually\n",
    "# Visualizing seeing how the data looks on the axis of a visualization\n",
    "fig1 = px.box(chicago_rawdata, y='CRASH_DATE')\n",
    "\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tidyness Issue 1 Programatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the data in Chicago dataset the `CRASH_DATE` is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chicago_rawdata[['CRASH_DATE']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to spilt the date and time in their columns respectively for tidiness purposes. Now lets look at the New york data below.....\n",
    "\n",
    "The New york dataset pertaining to this is more tidier and is also what is wanted from the Chicago dataset. The Newyork dataset as two columns separated already as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newyork_rawdata[['crash_date', 'crash_time']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from above New york has two columns `crash_date` and `crash_time` resectively. However, there are two sets of dates...Since the `crash_time` column has valid time stamps we will use and consider `crash_time` the valid column and perhaps drop the other column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### NOTE THIS IS A PROGRAMMATIC EXAMPLE WHAT TO EXPECT:\n",
    "(Not using chicago data, but just a test dataframe for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Setting up a test dataframe\n",
    "test_df_2 = pd.DataFrame({'my_timestamp': pd.date_range('2016-1-1 15:00', periods=5)})\n",
    "\n",
    "test_df_2['new_date'] = [d.date() for d in test_df_2['my_timestamp']]\n",
    "test_df_2['new_time'] = [d.time() for d in test_df_2['my_timestamp']]\n",
    "\n",
    "test_df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we want to separate the date and time columns in each of our `Newyork` and `Chicago` data respectively. \n",
    "This step will be done later on the datarames respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of ISSUE AND JUSTIFICATION of Tidiness ISSUE 1\n",
    "\n",
    "- From the above visualization the `CRASH_DATE` variable from the Chicago dataset appears on the y-axis. It can be seen how the data looks with the date and time. The objective is to separate the data and perhaps create a new column for date and time separately. Fina\n",
    "\n",
    "- Next we programmtically inspected the New york data and saw that the `crash_date` and `crash_time` properly separated so no tidying needs to be done there. \n",
    "\n",
    "- However, one of the columns in New york has missing time but has dates. While `crash_time` has both date and time stamps. So we will drop `crash_date`. \n",
    "\n",
    "- THe solution here for the CHicago data would be to separate the time and data. Have the date separate like so `01/13/2018` and time like so `02:43:00 PM`. The data will be placed in two different columns respectively. \n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ffMoRGSwzYj"
   },
   "source": [
    "### Tidiness Issue 2: IDENTIFYING COLUMNS RENAMING COLUMNS in each DATASET that Yield the same data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pause for a moment...if were to merge ourdatasets at the moment we would have more columns to deal with and more repepititive work to be done... Sometimes data accross two datasets is more common than you think but sometimes it is just the column names that throw us off. This is a indirect Tidiness issue that comes from working with two datasets. It is our job as the analyst to fix this!\n",
    "\n",
    "From observing the datasets while working on them we did notice many columns in each dataset are similar or compromise of similar data. For example just in the previous tidy issue **TIDINESS ISSUE 1** we noticed the two columns `crash_date` and `CRASH_DATE` respectively. Both columns are stating the same thing but both columns are named differently.Let us visualize below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUpeoqokw5Qt",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe visually and programmatically\n",
    "chicago_rawdata['CRASH_DATE'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8JK4DoXxtFA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "newyork_rawdata['crash_date'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said both columns yield the date/time data. However, the inconsistency that posses an issue here is the column names.There are more examples like this from each of the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chicago_rawdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "newyork_rawdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns_to_rename_chicago = chicago_rawdata[['CRASH_RECORD_ID', 'INJURIES_TOTAL', 'LATITUDE', 'LONGITUDE', 'LOCATION']]\n",
    "columns_to_rename_chicago.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns_to_rename_newyork = newyork_rawdata[['collision_id', 'number_of_persons_injured', 'latitude', 'longitude', 'location']]\n",
    "columns_to_rename_newyork.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chicago_rawdata.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above two lists. It can be seen that these columns have pretty much in common. Only the column names are differntiating them. So thinking ahead. We will re-name each of these columns with the same name so for example:\n",
    "\n",
    "The following Columns will be renamed like so in each dataset respectively:\n",
    "\n",
    "`CRASH_RECORD_ID` and `colision_id` will be renamed to `crash_collision_id`\n",
    "\n",
    "`INJURIES_TOTAL` and `number_of_persons_injured` will be renamed to `total_persons_injured`\n",
    "\n",
    " `LATITUDE` and `latitude` will be renamed to `latitude`\n",
    " \n",
    " `LONGITUDE` and `longitude` will be renamed to `longitude`\n",
    " \n",
    " `LOCATION` and `location` will be renamed to `location`\n",
    " \n",
    "`INJURIES_FATAL` and `number_of_persons_killed` will be renamed to `killed_by_accident`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summary of ISSUE AND JUSTIFICATION of Tidiness ISSUE 2\n",
    "\n",
    "- AS said earlier, we notice that there are some columns in each of the datasets that carry similar meaning. However, the column names are not the same.\n",
    "\n",
    "- Secondly, the capitalization of some columns and some not. For example it is noticed in the CHicago dataset the columns are in capital letters while the New york data is common letters. Here, we will stick the common letter format. to keep the dataset flowing on the same length. \n",
    "\n",
    "- Thirdly, we outlined the colomumns in each dataset that are similarly oriented with the same data outputs. AS said in the point above, common letters will be used. So therfore we will re-name the columns and change their capitalization.\n",
    "\n",
    "- Finally, all columns will be changed to common letters eventually when renamed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6gmLnBttpCh"
   },
   "source": [
    "## 3. Clean data\n",
    "Clean the data to solve the 4 issues corresponding to data quality and tidiness found in the assessing step. **Make sure you include justifications for your cleaning decisions.**\n",
    "\n",
    "After the cleaning for each issue, please use **either** the visually or programatical method to validate the cleaning was succesful.\n",
    "\n",
    "At this stage, you are also expected to remove variables that are unnecessary for your analysis and combine your datasets. Depending on your datasets, you may choose to perform variable combination and elimination before or after the cleaning stage. Your dataset must have **at least** 4 variables after combining the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First before we proceed we must make copies of our Original Datasets to avoid conflicts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# FILL IN - Make copies of the datasets to ensure the raw dataframes \n",
    "# are not impacted\n",
    "\n",
    "# Copy dataset one which is chicago data\n",
    "chicago_data_wrangled = chicago_rawdata.copy()\n",
    "\n",
    "# Copy dataset two which is new york data \n",
    "newyork_data_wrangled = newyork_rawdata.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmFhN52Yyn3l"
   },
   "source": [
    "### **Quality Issue 1: Removing Missing data **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will clean both datasets and drop the uncessary columns while dropping the missing values of the columns with missing data. We apply Pandas has two methods to do this:\n",
    "- `drop()` method. which drops the column entirely.\n",
    "- `dropna()` method. To clean the missing values of each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMINDER**: Remember in the assessment section we created thresholds... When running `isnull().sum()` we saw the values. Here we simply looked through that output and selected each column not falling with the thresgold for each dataset respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dropping columns from the chicago dataset\n",
    "chicago_data_wrangled = chicago_data_wrangled.drop(['RD_NO', 'CRASH_DATE_EST_I', 'LANE_CNT', 'INTERSECTION_RELATED_I',\n",
    "                                                   'NOT_RIGHT_OF_WAY_I', 'HIT_AND_RUN_I', 'HIT_AND_RUN_I',\n",
    "                                                   'PHOTOS_TAKEN_I', 'STATEMENTS_TAKEN_I', 'DOORING_I', 'WORK_ZONE_I',\n",
    "                                                   'WORK_ZONE_TYPE', 'WORKERS_PRESENT_I'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Dropping columns from the new york dataset\n",
    "\n",
    "newyork_data_wrangled = newyork_data_wrangled.drop(['on_street_name', 'off_street_name', 'vehicle_type_code2', 'borough', \n",
    "                                                    'zip_code', 'cross_street_name', 'contributing_factor_vehicle_3',\n",
    "                                                    'vehicle_type_code_3', 'contributing_factor_vehicle_4', \n",
    "                                                    'vehicle_type_code_4', 'contributing_factor_vehicle_5',\n",
    "                                                   'vehicle_type_code_5'], axis=1)\n",
    "\n",
    "\n",
    "print('COlumns successfully dropped!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we dropped the columns. Let us drop the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dropping missing rows for chicago data\n",
    "chicago_data_wrangled = chicago_data_wrangled.dropna()\n",
    "\n",
    "# Dropping missing rows for New york data\n",
    "newyork_data_wrangled = newyork_data_wrangled.dropna()\n",
    "\n",
    "print('Dropped Missing values!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Validating if cleaning was successful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chicago_data_wrangled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newyork_data_wrangled.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Justification: \n",
    "\n",
    "- We first identified the columns not falling within the thresholds and succesfully dropped them using `drop()`. Before dropping columns. Chicago data had `49` columns. After dropping it has `39` columns. WHile New york had `29` columns it now has `17` columns after dropping. \n",
    "\n",
    "\n",
    "- We used the Pandas method `dropna()` to drop the missing rows of data from each column within both datasets. \n",
    "\n",
    "\n",
    "- WE then used `info()` on both datasets again to validate our cleaning strategy to see if it was successful. Now we see some differences. For example in Chicago data we first started with `708268` rows now we have `682625` rows of data. While for new york we had `1000` rows . Now we have `711` rows. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_DAUbJrymBL"
   },
   "source": [
    "### **Quality Issue 2:Adding the default column with city name**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AS mentioned in the Assessment section we need to add a column identifying the city. This column will be a constant value as mentioned.Below we will create a column called `city_name` in each of ourdataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Yfb-Yu5MTuE",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#FILL IN - Apply the cleaning strategy\n",
    "\n",
    "# Creating the new default columns for our datasets\n",
    "\n",
    "# First with chicago\n",
    "chicago_data_wrangled['city_name'] = 'Chicago-City'\n",
    "\n",
    "# Then the newyork dataset\n",
    "newyork_data_wrangled['city_name'] = 'NewYork-City'\n",
    "\n",
    "print('New column created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Validating if columns were successfully created in the dataframes respectfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ionB2sRaMUmY",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#FILL IN - Validate the cleaning was successful\n",
    "\n",
    "# First chicago data\n",
    "chicago_data_wrangled[['CRASH_DATE', 'city_name']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now newyork data\n",
    "newyork_data_wrangled[['crash_date', 'city_name']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Justification: \n",
    "\n",
    "- The new column `city_name` was successfully created in both `chicago_data_wrangled` and `newyork_data_wrangled`datasets respectively. \n",
    "\n",
    "- When validating by outputs we do see that the `city_name` column in the chicago dataset outputs `Chicago-City`\n",
    "\n",
    "- Finally, while validating the output for New york we do see `city_name` output `NewYork-City`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIUrrfSNyOPR"
   },
   "source": [
    "### **Tidiness Issue 1: ** Separating the `CRASH_DATE`  in the Chicago dataset to create new columns for date and time respectively. Secondly, removing the uncessary date column `crash_date` from the New york dataset. In the New york dataset we continue by separating the `crash_time` column into date and time respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with Chicago....\n",
    "Lets convert the data type first to `date/time` as we need to separate the column to create two new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First convert the CRASH DATE column to date/time format \n",
    "chicago_data_wrangled['CRASH_DATE'] = pd.to_datetime(chicago_data_wrangled['CRASH_DATE'])\n",
    "print ('Successfully converted the `CRASH_DATE` to date/time format!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Below we can go ahead and split the `CRASH_DATE` column to separate the date and time into two new columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fib0zAm333bn",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#FILL IN - Apply the cleaning strategy\n",
    "\n",
    "# CHicago dataset\n",
    "chicago_data_wrangled['crash_date'] = [d.date() for d in chicago_data_wrangled['CRASH_DATE']]\n",
    "chicago_data_wrangled['crash_time'] = [d.time() for d in chicago_data_wrangled['CRASH_DATE']]\n",
    "\n",
    "print ('Successfully split `CRASH_DATE` into two new columns `crash_date` and `crash_time`!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets work on the New york data...\n",
    "\n",
    "First we drop the complex date column `crash_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newyork_data_wrangled = newyork_data_wrangled.drop(['crash_date'], axis=1)\n",
    "('Successfully dropped crash_date column!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will do what we did similar above to split the time and date into two separate columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newyork_data_wrangled['crash_time'] = pd.to_datetime(newyork_data_wrangled['crash_time'])\n",
    "print ('Successfully converted column to date/time format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newyork_data_wrangled['crash_date'] = [d.date() for d in newyork_data_wrangled['crash_time']]\n",
    "newyork_data_wrangled['crash_time'] = [d.time() for d in newyork_data_wrangled['crash_time']]\n",
    "\n",
    "print ('Successfully split the`crash_time` column into two new columns `crash_date` and `crash_time`!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Validating "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can validate that the `CRASH_DATE` column from chicago was successfully split and formed two new columns `crash_date` and `crash_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#FILL IN - Validate the cleaning was successful\n",
    "chicago_data_wrangled[['CRASH_DATE', 'crash_date', 'crash_time']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second we can look at the New york data to see the respective columns split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newyork_data_wrangled[['crash_date', 'crash_time']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Justification: \n",
    "\n",
    "- The first thing was to conver the `CRASH_DATE` column to pandas date/time format\n",
    "- Secondly, we had to split `CRASH_DATE` from chicago dataset to new columns `crash_date` and `crash_time`\n",
    "- Thirdly, we dropped the current `crash_date` column in Newyork dataset because the data seemed incomplete\n",
    "- Fourthly, we then re created the `crash_date` column by properly splitting the date and time respectively from the `crash_time` column\n",
    "- Finally, we now have a `crash_date` and `crash_time` column respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o51Bt8kwyTzk"
   },
   "source": [
    "### **Tidiness Issue 2: FILL IN**: Renaming columns in both datasets that yield the same data but just different column names. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the assessment section we had outlined a few columns that will be renamed in each dataframe. Each of these columns carry out the same data in their own dataframes respectivelly, but the only difference is their name and column heading. We can fix this issue to make the column names more tidy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zW8O5yx4Y9O",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#FILL IN - Apply the cleaning strategy\n",
    "\n",
    "# Starting with chicago data.\n",
    "chicago_data_wrangled = chicago_data_wrangled.rename({'CRASH_RECORD_ID': 'crash_collision_id',\n",
    "                                                      'INJURIES_TOTAL': 'total_persons_injured',\n",
    "                                                     'LATITUDE': 'latitude', 'LONGITUDE': 'longitude',\n",
    "                                                     'LOCATION': 'location', 'INJURIES_FATAL': 'killed_by_crash'}, axis=1)\n",
    "\n",
    "\n",
    "print ('Renaming the columns complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newyork_data_wrangled = newyork_data_wrangled.rename({'collision_id': 'crash_collision_id',\n",
    "                                                      'number_of_persons_injured': 'total_persons_injured',\n",
    "                                                     'LATITUDE': 'latitude', 'LONGITUDE': 'longitude',\n",
    "                                                     'LOCATION': 'location','number_of_persons_killed': 'killed_by_crash'}, axis=1)\n",
    "\n",
    "\n",
    "print ('Renaming the columns complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And complete we have renamed the necessary columns in each of the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Validating:\n",
    "\n",
    "I will just simply print the columns of the wrangled dataset VS the original ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6I_Sr7lxXi5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#FILL IN - Validate the cleaning was successful\n",
    "\n",
    "#first chicago\n",
    "\n",
    "chicago_column_list = chicago_rawdata.columns\n",
    "\n",
    "\n",
    "\n",
    "# Now the renamed list\n",
    "chicago_renamed_list = chicago_data_wrangled.columns\n",
    "\n",
    "\n",
    "print(chicago_column_list)\n",
    "print (chicago_renamed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#FILL IN - Validate the cleaning was successful\n",
    "\n",
    "#first chicago\n",
    "\n",
    "newyork_column_list = newyork_rawdata.columns\n",
    "\n",
    "\n",
    "\n",
    "# Now the renamed list\n",
    "newyork_renamed_list = newyork_data_wrangled.columns\n",
    "\n",
    "\n",
    "print(newyork_column_list)\n",
    "print (newyork_renamed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above list comparisons you can successfully see that the columns were successfully renamed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Justification: \n",
    "\n",
    "- The Columns were renamed using the `.rename` method from pandas in each dataset respectivelly\n",
    "\n",
    "- THe wrangled dataset columns were compared to the raw dataset columns to observe the difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remove unnecessary variables and combine datasets**\n",
    "\n",
    "Now we will not need all the variables and columns so here we will manually select the columns/variables required from each dataset. We will create two new datasets one called `chicago_wrangled_complete` and `newyork_wrangled_complete`. These will store the important variables going forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting up new dataset with important variables\n",
    "chicago_wrangled_complete = chicago_data_wrangled[['crash_collision_id', 'total_persons_injured', 'killed_by_crash',\n",
    "                                                 'latitude', 'longitude', 'location', 'city_name', 'crash_date', 'crash_time']]\n",
    "\n",
    "chicago_wrangled_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newyork_wrangled_complete = newyork_data_wrangled[['crash_collision_id', 'total_persons_injured', 'killed_by_crash',\n",
    "                                                   'latitude', 'longitude','location', 'city_name', \n",
    "                                                   'crash_date', 'crash_time']]\n",
    "\n",
    "newyork_wrangled_complete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our two completely wrangled datasets `chicago_wrangled_complete` and `newyork_wrangled_complete` focusing on the necessary variables we wish to analyze. Now we will merge both our datasets using `pd.concat` to create one new whole variable called `chicago_newyork_crash_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chicago_newyork_crash_data = pd.concat([chicago_wrangled_complete, newyork_wrangled_complete])\n",
    "\n",
    "chicago_newyork_crash_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F42urHuzttjF"
   },
   "source": [
    "## 4. Update your data store\n",
    "\n",
    "Here I will update and store the final version of the dataset. \n",
    "\n",
    "1. NOTE: The original raw datasets were already stored at the end of Section 1 earlier when the data was loaded.The raw datasets were stored in the following folder `/datasets`. The two data files stored in there are: `traffic-crashes-chicago.csv` and `dataset_two.csv` respectively. \n",
    "\n",
    "2. The final cleaned/merged dataset will be stored in `/final_cleaned_dataset`. The datafile name is `chicago_newyork_crash_data.csv`\n",
    "\n",
    "The final data will be converted and saved as a file CSV below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('final_cleaned_dataset/', exist_ok=True)\n",
    "chicago_newyork_crash_data.to_csv('final_cleaned_dataset/chicago_newyork_crash_data.csv')\n",
    "\n",
    "print ('Final merged and cleaned dataset has been successfully saved in folder final_cleaned_dataset!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGy_yddGtzhM"
   },
   "source": [
    "## 5. Answer the research question\n",
    "\n",
    "### **5.1:** Define and answer the research question \n",
    "Going back to the problem statement in step 1, use the cleaned data to answer the question you raised. Produce **at least** two visualizations using the cleaned data and explain how they help you answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a sample of the data I will plot some visualizations to answer the questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a sample segment \n",
    "sample_final_data = chicago_newyork_crash_data.iloc[-2000:]\n",
    "print('Sample created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjedE4s4ZkEd"
   },
   "source": [
    "**Research question 1: Which city has a higher kill count when crashes occur, Chicago or New york city?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lkw3rW9kZmOm",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Visual 1 - FILL IN\n",
    "\n",
    "ax = sb.countplot(data=sample_final_data, y=\"city_name\", hue='killed_by_crash')\n",
    "plt.title('The city that has the most kill count by Crashes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to research question 1:**\n",
    "  \n",
    "  From the above count plot it can be seen that Chicago City produces a higher kill count in general when accidents occur. THis means that the death rate in accidents is greater in Chicago city over New york City. Chicago City's count by persons who have been killed by crashed was exceeding 1200 counts. While New York was approaching 800. That is almost a difference of 500 counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Research question 2: Between both Chicago and NewYork. Which date or time of the year has recorded the highest fatalatiesin general between both cities?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sb.lineplot(data=sample_final_data, x=\"crash_date\", y=\"killed_by_crash\")\n",
    "plt.title('The date/year with the highest Fatalaties between both cities')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to research question 2:** \n",
    "\n",
    "From the above it can be seen that within the recent year of 2023. The month of March seemed to be a catastrophic month for both cities recording a 25% killing happening everytime a accident occured within either city. Data seemed more flatter earlier on in the year as it may have been winter and less driving was going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ezWXXZVj-TP"
   },
   "source": [
    "### **5.2:** Reflection\n",
    "In 2-4 sentences, if you had more time to complete the project, what actions would you take? For example, which data quality and structural issues would you look into further, and what research questions would you further explore?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XB3RBDG5kFe1"
   },
   "source": [
    "**My Reflection:** \n",
    "\n",
    "- There were more quality issues and structural issues for example the datasets were mis matched in size by a very large proportion. For example the Chicago dataset had tens of thousands of values while the Newyork dataset had approximately a 1000.\n",
    "- To fix the issue above. Feature scaling is a part of it. Feature scaling may have helped to bring the data more on the same scale. Inspite of dropping columns and dropping missing values to negate some skew. Feature scaling would greatly help with a vast dataset. \n",
    "- Another thing I would have loved to try is Feature Engineering. This dataset could maybe had offered more. Some of the dataset columns were dropped due to missing values. But sometimes those columns do carry weight and can provide information. Running a feature engineering method would have been ideal\n",
    "- I wanted to further my visualizations using `geopy`. Both datasets have longitude and lattitude coordinates. A future intention was to find a map of New york city and map of Chicago City and try plot the coordinates on a basemap so that we can visually see where the crashes occured in either city!\n",
    "- Finally, given the right scaling right feature engineering. I could have run some predictions and train some Machine Learning models. \n",
    "- And ask more questions such as Predicting crashes at a certain location in NEw york or CHicago? Predicting a crash rate with a chance of killing someone VS someone getting injured. These are more questions that could have been asked..\n",
    "- Finally, a project that now I will love to work on my free time to just explore the data further!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
